# @package _global_

# to execute this experiment run:
# python train.py experiment=example

defaults:
  - override /data: diffmotion_multi_test_datamodule.yaml
  - override /model: diffmotion_multi_test_module.yaml
  - override /logger: null # wandb.yaml
  - override /callbacks: callbacks_trinity_diffusion.yaml
#  - override /trainer: ddp.yaml
  - override /trainer: Trinity_gpu.yaml
  - override /paths: trinity_diffmotion_path.yaml

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters
task_name: "BEAT_W_based_generate"
use_auto_scale_batch_size: False
#num_sanity_val_steps: 0
#reload_dataloaders_every_epoch: False
tags: ["BEAT_W_based_generate"]

ckpt_path: # set the pre-trained model path here!! like: /path/to/your/checkpoint.ckpt
#ckpt_path: null
seed: null
train: False
test: True

logger:
  wandb:  # we use wandb for logging, please find instructions to set it up from https://docs.wandb.ai/quickstart, if you want to use wandb or other loggers, please set parameter: - override /logger:
    # when you have login in the wandb, please copy the API key to the parameters in the train_diffmotion.py line 80.
    id:  ${task_name} # pass correct id to resume experiment! ${task_name}
#    tags: ${tags}
    name: ${task_name}
    project: "BEAT"
    group: "BEAT_W_based_generate"

trainer:
  min_epochs: 10
  max_epochs: 100
#  max_steps: 10
  devices: [0]
  gradient_clip_val: 0.5
  check_val_every_n_epoch: 5
  log_every_n_steps: 20
#  accumulate_grad_batches: 8
  fast_dev_run: False
#  profiler: "simple"  # "advanced" "simple"
#  precision: 16  # worst

profiler:
  _target_: pytorch_lightning.profilers.SimpleProfiler
  extended: True
#  dirpath: ${paths.output_dir}/
#  filename: 'profiler'

model:
  param_for_name: ""
  timesteps: 1000 #2000
  beta_schedule: 'linear'  # cosine / linear
  linear_start: 1e-4
  linear_end: 8e-2    # 5e-2

  gesture_features: 156 # 69 65
  encoder_dim: 1280   # 1024
  learning_rate: 2.0e-4
  sampler: "DDPM" #'DDIM'/'DDPM',/'dpmsolver++'/'dpmsolver'
  ddim_steps: 200 # 10 ~ 100 for timesteps=500
  num_sequences: 1  # each audio sequence has 1 gesture sequence, you can set it to more than 1 for multi-gesture sequence each audio sequence
  eps_theta_mod:
    _target_: src.diffmotion.diffmotion_epsilon_net.diffmotion_epsilon_theta.TrinityEpsilonTheta
    atten_sel: 'conformer'          # conformer 'informer', 'full_transformer'

    mask_sel: 'causalMask'          # ‘diagonal_matrix’, 'no_mask', 'causalMask','dman'
    dman_mask: 'no_mask'
    dman_max_len: 400               # max_seq_len 120, 100 200 300 400
    dman_position: False
    inf_pos: False
    informer_factor: 5
    upper_offset: 10
    lower_offset: -10
#    re_weight_m: 1
    block_depth: 12                 # 4 is default 6  12 layers with 8 heads
    num_att_heads: 16               # 8  16 32  64
    motion_encoder:
      encoder_type: 'single_conv' # 'single_conv' 'separable_conv'
      encoder_kernel_size: 3
    wavLMEncoder:
      checkpoint_path: ${paths.root_dir}/src/utils/wavlm/pretrain-models/WavLM-Base+.pt
      # checkpoint_path: '/home/zf223669/Mount/Diffmotion-v3-sync/src/utils/wavlm/pretrain-models/WavLM-Large.pt'
    cond_dropout_rate: 0.1
    conv_depthwise: False
    use_DropKey: False
  scheduler_config: # 10000 warmup steps
    target: src.utils.LDM.lr_scheduler.LambdaLinearScheduler
    params:
      warm_up_steps: [ 1000 ]
      cycle_lengths: [ 10000000000000 ] # incredibly large number to prevent corner cases
      f_start: [ 8.e-5 ]
      f_max: [ 1. ]
      f_min: [ 1. ]
paths:
  data_dir: ${paths.root_dir}/data/BEAT/processed_20s
data:
  data_root: ${paths.data_dir}/feat_20fps_20s_WithExp_waveform_WithStd
  pin_memory: True
  batch_size: 38
  num_workers: 32
  is_smoothing: False
  test_style: ['1_wayne_0_39_39','10_kieks_0_9_9','11_nidal_0_66_66','12_zhao_0_88_88','13_lu_0_9_9','14_zhang_1_3_3','15_carlos_0_2_2','16_jorge_0_65_65','17_itoi_0_111_111','18_daiki_0_73_73','19_jaime_0_96_96','2_scott_0_109_109','20_li_0_95_95','21_ayana_0_82_82','22_luqi_0_88_88','23_hailing_0_10_10','24_kexin_0_111_111','25_goto_0_74_74','26_reamey_0_3_3','27_yingqing_0_9_9','28_tiffnay_0_74_74','29_hanieh_0_4_4','3_solomon_0_46_46','30_katya_0_81_81','4_lawrence_0_5_5','5_stewart_0_9_9','6_carla_0_96_96','7_sophie_0_6_6','8_catherine_0_82_82','9_miranda_1_12_12']


